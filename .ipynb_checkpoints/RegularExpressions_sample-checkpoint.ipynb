{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiger\n"
     ]
    }
   ],
   "source": [
    "#import dependecies\n",
    "import re\n",
    "string = 'Tiger is the national animal of india'\n",
    "pattern ='Tiger'\n",
    "result = re.match(pattern,string).group(0)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiger\n"
     ]
    }
   ],
   "source": [
    "string = 'The national animal of india is Tiger'\n",
    "pattern ='Tiger'\n",
    "# result = re.findall(pattern,string)\n",
    "result = re.search(pattern,string).group(0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['national', 'national']\n"
     ]
    }
   ],
   "source": [
    "string =\"national animal is tiger and national sport is hockey\"\n",
    "pattern =\"national\"\n",
    "result =re.findall(pattern,string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12-05-2007', '11-11-2011', '12-01-2009']\n"
     ]
    }
   ],
   "source": [
    "string =\"John 34-3456 12-05-2007,XYZ 56-4532 11-11-2011,ABC 67-8945 12-01-2009\"\n",
    "pattern =r'\\d{2}-\\d{2}-\\d{4}'\n",
    "result =re.findall(pattern,string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', '', 'a', 'sample', 'text', 'string']\n"
     ]
    }
   ],
   "source": [
    "string = 'this is; a,sample,text,string'\n",
    "pattern =r'[;,\\s]'\n",
    "result= re.split(pattern,string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket is a popular sport of the world\n"
     ]
    }
   ],
   "source": [
    "string =\"cricket is a popular sport of india\"\n",
    "pattern = \"india\"\n",
    "replacement= \"the world\"\n",
    "result = re.sub(pattern,replacement,string)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# pattern gives the index number\n",
    "string =\"national animal is tiger and national sport is hockey\"\n",
    "pattern =\"national\"\n",
    "result =re.finditer(pattern,string)\n",
    "for i in result:\n",
    "    print(i.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John 34-3456 Monday,XYZ 56-4532 Monday,ABC 67-8945 Monday\n"
     ]
    }
   ],
   "source": [
    "string =\"John 34-3456 12-05-2007,XYZ 56-4532 11-11-2011,ABC 67-8945 12-01-2009\"\n",
    "pattern =r'\\d{2}-\\d{2}-\\d{4}'\n",
    "result = re.findall(pattern,string)\n",
    "# print(result)\n",
    "print(re.sub(pattern,\"Monday\",string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 50), match='training_queries@analyticsvidhya.com'>\n"
     ]
    }
   ],
   "source": [
    "string =\"contact us on training_queries@analyticsvidhya.com\"\n",
    "pattern = '([\\w.-]+)@([\\w.-]+)'\n",
    "result = re.search(pattern,string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Viswajani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Do I love cricket?i play cricket.i like TeamIndia']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "text =\"Do I love cricket?i play cricket.i like TeamIndia\"\n",
    "\n",
    "sent_tokenize(text)\n",
    "#word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " 'I',\n",
       " 'love',\n",
       " 'cricket',\n",
       " '?',\n",
       " 'i',\n",
       " 'play',\n",
       " 'cricket.i',\n",
       " 'like',\n",
       " 'TeamIndia']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "win\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"playing\"))\n",
    "print(stemmer.stem(\"winning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "winner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Viswajani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#lem\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "print(lemm.lemmatize(\"increase\"))\n",
    "print(lemm.lemmatize(\"winner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Viswajani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('D', 'NNP'),\n",
       " ('o', 'MD'),\n",
       " (' ', 'VB'),\n",
       " ('I', 'PRP'),\n",
       " (' ', 'VBP'),\n",
       " ('l', 'JJ'),\n",
       " ('o', 'NN'),\n",
       " ('v', 'NN'),\n",
       " ('e', 'NN'),\n",
       " (' ', 'NNP'),\n",
       " ('c', 'VBZ'),\n",
       " ('r', 'NN'),\n",
       " ('i', 'JJ'),\n",
       " ('c', 'VBP'),\n",
       " ('k', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('?', '.'),\n",
       " ('i', 'JJ'),\n",
       " (' ', 'VBP'),\n",
       " ('p', 'JJ'),\n",
       " ('l', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('y', 'NN'),\n",
       " (' ', 'NNP'),\n",
       " ('c', 'VBZ'),\n",
       " ('r', 'NN'),\n",
       " ('i', 'JJ'),\n",
       " ('c', 'VBP'),\n",
       " ('k', 'NN'),\n",
       " ('e', 'NN'),\n",
       " ('t', 'NN'),\n",
       " ('.', '.'),\n",
       " ('i', 'JJ'),\n",
       " (' ', 'VBP'),\n",
       " ('l', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('k', 'VBP'),\n",
       " ('e', 'NN'),\n",
       " (' ', 'NNP'),\n",
       " ('T', 'NNP'),\n",
       " ('e', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('m', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('n', 'VBP'),\n",
       " ('d', 'JJ'),\n",
       " ('i', 'NN'),\n",
       " ('a', 'DT')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Part of speech tag(pos)\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "text =\"Do I love cricket?i play cricket.i like TeamIndia\"\n",
    "token =word_tokenize(text)\n",
    "pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21'),\n",
       " Synset('well.r.01'),\n",
       " Synset('thoroughly.r.02')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x000001F4697F6EC8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence =\"I love to play FootBall\"\n",
    "n=2\n",
    "ngrams(word_tokenize(sentence),n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love', 'to')\n",
      "('love', 'to', 'play')\n",
      "('to', 'play', 'FootBall')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence =\"I love to play FootBall\"\n",
    "n=3\n",
    "for ngram in ngrams(word_tokenize(sentence),n):\n",
    "    print (ngram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
